\chapter{Algorithms} \label{chap:algorithms}

This chapter gives an overview over the available algorithms in {\ViennaCL}.
The focus of {\ViennaCL} is on iterative solvers, for which {\ViennaCL} provides a generic implementation that allows the use of the same code on the CPU (either using \ublas, Eigen, MTL4 or \OpenCL) and on the GPU (using \OpenCL).

\section{Direct Solvers} \label{sec:direct-solvers}
{\ViennaCLversion} provides triangular solvers and LU factorization without pivoting for the solution of dense linear systems. The interface is similar to that of {\ublas}

\begin{lstlisting}
  using namespace viennacl::linalg;  //to keep solver calls short
  viennacl::matrix<float>  vcl_matrix;
  viennacl::vector<float>  vcl_rhs;
  viennacl::vector<float>  vcl_result;

  /* Set up matrix and vectors here */

  //solution of an upper triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs, upper_tag());
  //solution of a lower triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs, lower_tag());

  //solution of a full system right into the load vector vcl_rhs:
  lu_factorize(vcl_matrix);
  lu_substitute(vcl_matrix, vcl_rhs);    
\end{lstlisting}
In {\ViennaCLminorversion} there is no pivoting included in the LU factorization
process, hence the computation may break down or yield results with poor
accuracy. However, for certain classes of matrices (like diagonal dominant
matrices) good results can be obtained without pivoting.

It is also possible to solve for multiple right hand sides:
\begin{lstlisting}
  using namespace viennacl::linalg;  //to keep solver calls short
  viennacl::matrix<float>  vcl_matrix;
  viennacl::matrix<float>  vcl_rhs_matrix;
  viennacl::matrix<float>  vcl_result;

  /* Set up matrices here */

  //solution of an upper triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs_matrix, upper_tag());

  //solution of a lower triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs_matrix, lower_tag());
\end{lstlisting}


\section{Iterative Solvers} \label{sec:iterative-solvers}
{\ViennaCL} provides different iterative solvers for various classes of
matrices, listed in Tab.~\ref{tab:linear-solvers}. Unlike direct solvers, the
convergence of iterative solvers relies on certain properties of the system
matrix. Keep in mind that an iterative solver may fail to converge, especially
if the matrix is ill conditioned or a wrong solver is chosen. 

\TIP{For full details on linear solver calls, refer to the reference
documentation located in \texttt{doc/doxygen/} and to the tutorials}

\TIP{The iterative solvers can directly be used for {\ublas}, Eigen and MTL4 objects! Please have a look at Chap.~\ref{chap:other-libs} and the respective tutorials in the examples/tutorials/ folder.}

\NOTE{In {\ViennaCLversion}, GMRES using ATI GPUs yields wrong results due to a bug in Stream SDK v2.1. Consider using newer versions of the Stream SDK.}

\begin{lstlisting}
viennacl::compressed_matrix<float>  vcl_matrix;
viennacl::vector<float>  vcl_rhs;
viennacl::vector<float>  vcl_result;

/* Set up matrix and vectors here */

//solution using conjugate gradient solver:
vcl_result = viennacl::linalg::solve(vcl_matrix,
				     vcl_rhs,
				     viennacl::linalg::cg_tag());

//solution using BiCGStab solver:
vcl_result = viennacl::linalg::solve(vcl_matrix,
				     vcl_rhs,
				     viennacl::linalg::bicgstab_tag());

//solution using GMRES solver:
vcl_result = viennacl::linalg::solve(vcl_matrix,
				     vcl_rhs,
				     viennacl::linalg::gmres_tag());
\end{lstlisting}

\begin{table}[tb]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{p{4cm}|p{3cm}|p{7.5cm}}
Method & Matrix class & ViennaCL\\
\hline
Conjugate Gradient (CG) & symmetric positive definite & \texttt{y = solve(A, x, cg\_tag());} \\
Stabilized Bi-CG (BiCGStab) & non-symmetric & \texttt{y = solve(A, x, bicgstab\_tag());} \\
Generalized Minimum Residual (GMRES) & general & \texttt{y = solve(A, x, gmres\_tag());} \\
\hline
\end{tabular}
\caption{Linear solver routines in {\ViennaCL} for the computation of $y$ in the expression $Ay = x$ with given $A$, $x$.}
\label{tab:linear-solvers}
\end{center}
\end{table}

Customized error tolerances can be set in the solver tags. The convention is
that solver tags take the relative error tolerance as first argument and the
maximum number of iteration steps as second argument. Furthermore, after the
solver run the number of iterations and the estimated error can be obtained from
the solver tags as follows:
\begin{lstlisting}
// conjugate gradient solver with tolerance 1e10
// and at most 100 iterations:
viennacl::linalg::cg_tag custom_cg(1e-10, 100);
vcl_result = viennacl::linalg::solve(vcl_matrix, vcl_rhs, custom_cg);
//print number of iterations taken and estimated error:
std::cout << "No. of iters: " << custom_cg.iters() << std::endl;
std::cout << "Est. error: " << custom_cg.error() << std::endl;
\end{lstlisting}
The BiCGStab solver tag can be customized in exactly the same way. The GMRES
solver tag takes as third argument the dimension of the Krylov space. Thus, a
tag for GMRES(30) with tolerance $1\mathrm{E}\!-\!10$ and at most $100$ total
iterations
(hence, up to three restarts) can be set up by 
\begin{lstlisting}
viennacl::linalg::gmres_tag custom_gmres(1e-10, 100, 30);
\end{lstlisting}

\section{Preconditioners} \label{sec:preconditioner}
{\ViennaCL} ships with a generic implementation of several preconditioners.
The preconditioner setup is expect for simple diagonal preconditioners always carried out on the CPU host due to the need for dynamically allocating memory.
Thus, one may not obtain an overall performance benefit if too much time is spent on the preconditioner setup.

\TIP{The preconditioner also works for {\ublas} types!}

In the following it is assumed that the sparse linear system of equations is given as follows:
\begin{lstlisting}
using viennacl::compressed_matrix;

typedef compressed_matrix<float>   SparseMatrix;
SparseMatrix  vcl_matrix;
viennacl::vector<float>  vcl_rhs;
viennacl::vector<float>  vcl_result;

/* Set up matrix and vectors here */
\end{lstlisting}

% \begin{table}[tb]
% \begin{center}
% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{p{3cm}|p{4cm}|p{7cm}}
% Method & Brief description & Parameters\\
% \hline
% ILUT & incomplete LU factorization & First parameter: Maximum number of entries
% per row. Second parameter: Drop tolerance. \\
% Jacobi & Divide each row in $A$ by its diagonal entry & none \\
% Row Scaling & Divide each row in $A$ by its norm & First parameter specifies
% the norm (1: $l^1$-norm, 2: $l^2$-norm)\\
% \hline
% \end{tabular}
% \caption{Preconditioners for iterative solvers in {\ViennaCL}.}
% \label{tab:preconditioners}
% \end{center}
% \end{table}

\subsection{Incomplete LU Factorization with Threshold (ILUT)}
The incomplete LU factorization preconditioner aims at computing sparse matrices lower and upper triangular matrices $L$ and $U$ such that the sparse system
matrix is approximately given by $A \approx LU$. In order to control the sparsity pattern of $L$ and $U$, a threshold strategy is used (ILUT)
\cite{saad-iterative-solution}. Due to the serial nature of the preconditioner, the setup as well as each application of ILUT to the residual is computed on
the CPU.

\begin{lstlisting}
//compute ILUT preconditioner:
ilut_precond< SparseMatrix > vcl_ilut(vcl_matrix,
                                      viennacl::linalg::ilut_tag());

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     vcl_ilut);   //preconditioner here
\end{lstlisting}
Two parameters can be passed to the constructor of \lstinline|ilut_tag|: The first specifies the maximum number of entries per row in $L$ and $U$, while the
second parameter specifies the drop tolerance.

\subsection{Incomplete LU Factorization with Static Pattern (ILU0)}
Similar to ILUT, ILU0 computes an approximate LU factorization with sparse factors L and U.
While ILUT determines the location of nonzero entries on the fly, ILU0 uses the sparsity pattern of A for the sparsity pattern of L and U \cite{saad-iterative-solution}.
Due to the serial nature of the preconditioner, the setup as well as each application of ILU0 to the residual is computed on
the CPU.

\begin{lstlisting}
//compute ILU0 preconditioner:
ilu0_precond< SparseMatrix > vcl_ilu0(vcl_matrix,
                                      viennacl::linalg::ilu0_tag());

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     vcl_ilut);   //preconditioner here
\end{lstlisting}
Two parameters can be passed to the constructor of \lstinline|ilu0_tag|:
The first parameter specifies the lower row and column index for which ILU0 should be computed, while the second parameter specifies the upper bound on the row and column indices considered.
For example, the parameter set $(2,5)$ is supplied, ILU0 is computed for the diagonal block $A(2:4, 2:4)$, where $2:4 = \{2, 3, 4 \}$.
By default, ILU0 is computed for the full system matrix.

\subsection{Block-ILU}
To overcome the serial nature of ILUT and ILU0 applied to the full system matrix,
a parallel variant is to apply ILU to diagonal blocks of the system matrix.
This is accomplished by the \lstinline|block_ilu| preconditioner, which takes
the system matrix type as first template argument and the respective ILU-tag type as second template argument
(either \lstinline|ilut_tag| or \lstinline|ilu0_tag|). 

\begin{lstlisting}
//compute block-ILU preconditioner using ILU0 for each block:
block_ilu_precond<SparseMatrix,
                  ilu0_tag> vcl_block_ilu0(vcl_matrix,
                                           ilu0_tag());

//solve
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     vcl_block_ilu0);
\end{lstlisting}
A third argument can be passed to the constructor of \lstinline|block_ilu_precond|: 
Either the number of blocks to be used (defaults to 4), or an index vector with fine-grained control over the blocks. Refer to the Doxygen pages in doc/doxygen for details.


\subsection{Jacobi Preconditioner}
A Jacobi preconditioner is a simple diagonal preconditioner given by the reciprocals of the diagonal entries of the system matrix $A$.
Use the preconditioner as follows:
\begin{lstlisting}
//compute Jacobi preconditioner:
jacobi_precond< SparseMatrix > vcl_jacobi(vcl_matrix,
                                          viennacl::linalg::jacobi_tag());

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::cg_tag(),
                                     vcl_jacobi);
\end{lstlisting}


\subsection{Row Scaling Preconditioner}
A row scaling preconditioner is a simple diagonal preconditioner given by the reciprocals of the norms of the rows of the system matrix $A$.
Use the preconditioner as follows:
\begin{lstlisting}
//compute row scaling preconditioner:
row_scaling< SparseMatrix > vcl_row_scaling(vcl_matrix,
                                      viennacl::linalg::row_scaling_tag());

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::cg_tag(),
                                     vcl_row_scaling);
\end{lstlisting}
The tag \lstinline|viennacl::linalg::row_scaling_tag()| can be supplied with a parameter denoting the norm to be used. A value of \lstinline|1| specifies the
$l^1$-norm, while a value of $2$ selects the $l^2$-norm (default).

\subsection{Algebraic Multigrid}
\NOTE{Algebraic Multigrid preconditioners are experimental in {\ViennaCLversion}. Interface changes as well as considerable performance improvements may
be included in future releases!}

\NOTE{Algebraic Multigrid preconditioners depend on {\ublas}.}

Algebraic multigrid mimics the behavior of geometric multigrid on the algebraic level and is thus suited for black-box purposes, where only the system matrix
and the right hand side vector are available \cite{trottenberg:multigrid}. Many different flavors of the individual multigrid ingredients exists
\cite{yang:parallel-amg}, of which the most common ones are implemented in {\ViennaCL}.

The two main ingredients of algebraic multigrid are a coarsening algorithm and an interpolation algorithm. The available coarsening methods are listed in
Tab.~\ref{tab:amg-coarsening}.
\begin{table}[tbp]
\begin{center}
\begin{tabular}{l|l}
Description & {\ViennaCL} option constant \\
\hline
Classical Ruge-St\"uben (RS) & \lstinline|VIENNACL_AMG_COARSE_RS| \\
One-Pass & \lstinline|VIENNACL_AMG_COARSE_ONEPASS| \\
RS0 & \lstinline|VIENNACL_AMG_COARSE_RS0| \\
RS3 & \lstinline|VIENNACL_AMG_COARSE_RS3| \\
Aggregation & \lstinline|VIENNACL_AMG_COARSE_AG| \\
Smoothed aggregation & \lstinline|VIENNACL_AMG_COARSE_SA| \\
\end{tabular}
\caption{AMG coarsening methods available in {\ViennaCL}. Per default, classical RS coarsening is used.\label{tab:amg-coarsening}}
\end{center}
\end{table}
The available interpolation methods are given in Tab.~\ref{tab:amg-interpolation}.
\begin{table}[tbp]
\begin{center}
\begin{tabular}{l|l}
Description & {\ViennaCL} option constant \\
\hline
Direct & \lstinline|VIENNACL_AMG_INTERPOL_DIRECT| \\
Classic & \lstinline|VIENNACL_AMG_INTERPOL_ONEPASS| \\
RS0 coarsening & \lstinline|VIENNACL_AMG_INTERPOL_RS0| \\
RS3 coarsening & \lstinline|VIENNACL_AMG_INTERPOL_RS3| \\
\end{tabular}
\caption{AMG interpolation methods available in {\ViennaCL}. Per default, direct interpolation is used.\label{tab:amg-interpolation}}
\end{center}
\end{table}
In addition, the following parameters can be controlled in the \lstinline|amg_tag| and can be passed to the constructor:
\begin{itemize}
 \item Strength of dependence threshold (default: $0.25$)
 \item Interpolation weight (default: $1$)
 \item Jacobi smoother weight (default: $1$)
 \item Number of pre-smoothing steps (default: $1$)
 \item Number of post-smoothing steps (default: $1$)
 \item Number of coarse levels
\end{itemize}

\TIP{Note that the efficiency of the various AMG flavors are typically highly problem-specific. Therefore, failure of one method for a particular problem does
NOT imply that other coarsening or interpolation strategies will fail as well.}

\subsection{Sparse Approximate Inverses}

\NOTE{Sparse Approximate Inverse preconditioners are experimental in {\ViennaCLversion}. Interface changes as well as considerable performance improvements may
be included in future releases!}

\NOTE{Sparse Approximate Inverse preconditioners depend on {\ublas}.}

An alternative construction of a preconditioner for a sparse system matrix $A$ is to compute a matrix $M$ with a prescribed sparsity pattern such that
\begin{align}
 \Vert AM - I \Vert_F \rightarrow \min \ ,
\end{align}
where $\Vert \cdot \Vert_F$ denotes the Frobenius norm.
This is the basic idea of sparse approximate inverse (SPAI) preconditioner. It becomes increasingly attractive because of their inherent high degree of
parallelism, since the minimization problem can be solved independently for each column of $M$. {\ViennaCL} provides two preconditioners of
this family: The first is the classical SPAI algorithm as described by Grote and Huckle \cite{grote:spai}, the second is the factored SPAI (FSPAI) for symmetric
matrices as proposed by Huckle \cite{huckle:fspai}.

SPAI can be employed for a CPU matrix \lstinline|M| of type \lstinline|MatrixType| as follows:
\begin{lstlisting}
// setup SPAI preconditioner, purely CPU-based
viennacl::linalg::spai_precond<MatrixType> 
  spai_cpu(M, viennacl::linalg::spai_tag(1e-3, 3, 5e-2));

//solve (e.g. using stab. Bi-conjugate gradient solver)
vcl_result = viennacl::linalg::solve(M,
                                     rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     spai_cpu);
\end{lstlisting}
The first parameter denotes the residual norm threshold for the full matrix, the second parameter the maximum number of pattern updates, and the third
parameter is the threshold for the residual of each minimization problem.

For GPU-matrices, only parts of the setup phase are computed on the CPU, because compute-intensive tasks can be carried out on the GPU:
\begin{lstlisting}
// setup SPAI preconditioner, GPU-assisted
viennacl::linalg::spai_precond<GPUMatrixType> 
  spai_gpu(vcl_matrix, viennacl::linalg::spai_tag(1e-3, 3, 5e-2));

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     spai_gpu);
\end{lstlisting}
The \lstinline|GPUMatrixType| is typically a \lstinline|viennacl::compressed_matrix| type.

For symmetric matrices, FSPAI can be used with the conjugate gradient solver:
\begin{lstlisting}
viennacl::linalg::fspai_precond<MatrixType> fspai_cpu(M, viennacl::linalg::fspai_tag());

//solve (e.g. using stab. Bi-conjugate gradient solver)
vcl_result = viennacl::linalg::solve(M,
                                     rhs,
                                     viennacl::linalg::cg_tag(),
                                     fspai_cpu);
\end{lstlisting}
Our experience is that FSPAI is typically more efficient than SPAI when applied to the same matrix, both in computational effort and in terms of convergence
acceleration of the iterative solvers. 

\NOTE{At present, there is no GPU-accelerated FSPAI included in {\ViennaCL}.}

Note that FSPAI depends on the ordering of the unknowns, thus bandwidth reduction algorithms may be employed first, cf.~Sec.~\ref{sec:bandwidth-reduction}.


\section{Fast Fourier Transform}
\NOTE{The fast Fourier transform is experimental in {\ViennaCLversion}. Interface changes as well as considerable performance improvements may
be included in future releases!}

Since there is no standardized complex type in {\OpenCL} at the time of the release of {\ViennaCLversion}, vectors need to be set up with real- and imaginary
part before computing a fast Fourier tranform (FFT). In order to store complex numbers $z_0$, $z_1$, etc.~in a \lstinline|viennacl::vector|, say \lstinline|v|,
the real and imaginary parts are mapped to even and odd entries of \lstinline|v| respectively: \lstinline|v[0] = Real(z_0)|, \lstinline|v[1] = Imag(z_0)|,
\lstinline|v[2] = Real(z_1)|, \lstinline|v[3] = Imag(z_1)|, etc.

The FFT of \lstinline|v| can then be computed either by writing to a second vector \lstinline|output| or by directly writing the result to \lstinline|v|
\begin{lstlisting}
 viennacl::fft(v, output);
 viennacl::inplace_fft(v);
\end{lstlisting}
Conversely, the inverse FFT is computed as
\begin{lstlisting}
 viennacl::ifft(v, output);
 viennacl::inplace_ifft(v);
\end{lstlisting}

\NOTE{In {\ViennaCLversion} the FFT with complexity $N \log N$ is computed for vectors with a size of a power of two only. For other vector sizes, a standard
discrete Fourier transform with complexity $N^2$ is employed. This is subject to change in future versions.}

\section{Bandwidth Reduction} \label{sec:bandwidth-reduction}
\NOTE{Bandwidth reduction algorithms are experimental in {\ViennaCLversion}. Interface changes as well as considerable performance improvements may
be included in future releases!}

The bandwidth of a sparse matrix is defined as the maximum difference of the indices of nonzero entries in a row, taken over all rows. A low bandwidth
typically allows for the use of efficient banded matrix solvers instead of iterative solvers. Moreover, better cache utilization as well as lower fill-in in
LU-factorization based algorithms can be expected.

For a given sparse matrix with large bandwidth, {\ViennaCL} provides routines for renumbering the unknowns such that the reordered system matrix shows much
smaller bandwidth. Typical applications stem from the discretization of partial differential equations by means of the finite element or the finite difference
method. The algorithms employed are as follows:
\begin{itemize}
 \item Classical Cuthill-McKee algorithm \cite{cuthill:reducing-bandwidth}
 \item Modified Cuthill-McKee algorithm \cite{cuthill:reducing-bandwidth}
 \item Gibbs-Poole-Stockmeyer algorithm, cf.~\cite{lewis:gps-algorithm}
\end{itemize}
The modified Cuthill-McKee algorithm also takes nodes with small, but not necessarily minimal degree as root node into account and may lead to better results
than the classical Cuthill-McKee algorithm. A parameter $a \in [0,1]$ controls the number of nodes considered: All nodes with degree $d$ fulfilling
\begin{align*}
 d_{\min} \leq d \leq d_{\min} + a(d_{\max} - d_{\min})
\end{align*}
are considered, where $d_{\min}$ and $d_{\max}$ are the miminum and maximum nodal degrees in the graph. A second parameter \lstinline|gmax| specifies the
number of additional root nodes considered.

The algorithms are called for a \lstinline|matrix| of a type compatible with \lstinline|std::vector< std::map<int, double> >| by
\begin{lstlisting}
 r = viennacl::reorder(matrix, viennacl::cuthill_mckee_tag());
 r = viennacl::reorder(matrix, 
                       viennacl::advanced_cuthill_mckee_tag(a, gmax));
 r = viennacl::reorder(matrix, viennacl::gibbs_poole_stockmeyer_tag());
\end{lstlisting}
and return the permutation array. In {\ViennaCLversion}, the user then needs to manually reorder the sparse matrix based on the permutation array. Example code
can be found in \lstinline|examples/tutorial/bandwidth-reduction.cpp|.


\section{Eigenvalue Computations}
%{\ViennaCL} 
Two algorithms for the computations of the eigenvalues of a matrix $A$ are implemented in {\ViennaCL}:
\begin{itemize}
\item The Power Iteration \cite{golub:matrix-computations}
\item The Lanczos Algorithm \cite{simon:lanczos-pro}
\end{itemize}
Depending on the parameter \lstinline|tag| either one of them is called. 
Both algorithms can be used for either {\ublas} or {\ViennaCL} compressed matrices.\\
In order to get the eigenvalue with the greatest absolut value the power iteration should be called. \\
The lanczos algorithm returns a vector of the largest eigenvalues with the same type as the entries of the matrix.

The algorithms are called for a matrix object \lstinline|A| by
\begin{lstlisting}
std::vector<double> largest_eigenvalues = viennacl::linalg::eig(A, ltag);
double largest_eigenvalue = viennacl::linalg::eig(A, ptag);
\end{lstlisting}


\subsection{Power Iteration}
The Power iteration aims at computing the eigenvalues of a matrix by calculating the product of the matrix and a vector for several times, where the resulting vector is used for the next product of the matrix and so on. The computation stops as soon as the norm of the vector converges. \\
The final vector is the eigenvector to the eigenvalue with the greatest absolut value.\\
To call this algorithm, \lstinline|piter_tag| must be used.
This tag has only one parameter: \\ \lstinline|terminationfactor| defines the accuracy of the computation, i.e. if the new norm of the eigenvector changes less than this parameter the computation stops and returns the corresponding eigenvalue (default: $1e-10$).\\
The call of the constructor may look like the following:
\begin{lstlisting} 
viennacl::linalg::piter_tag ptag(1e-8);
\end{lstlisting}

\TIP{Example code can be found in \lstinline|examples/tutorial/power-iter.cpp|.}

\subsection{The Lanczos Algorithm}
In order to compute the eigenvalues of a sparse high-dimensional matrix the lanczos algorithm can be used to find these. 
This algorithm reformulates the given high-dimensional matrix in a way such that the matrix can be rewritten in a tridiagonal matrix at much lower dimension. The eigenvalues of this tridiagonal matrix are equal to the largest eigenvalues of the original matrix. \\
The eigenvalues of the tridiagonal matrix are calculated by using the bisection method \cite{golub:matrix-computations}. \\
To call this lanczos algorithm, \lstinline|lanczos_tag| must be used.
This tag has several parameters that can be passed to the constructor:

\begin{itemize}
 \item The exponent of epsilon for the tolerance of the reorthogonalization, defined by the parameter \lstinline|factor| (default: $0.75$)
 \item The method of the lanczos algorithm: $0$ uses partial reorthogonalization, $1$ full reothogonalization and $2$ does not use reorthogonalization (default: $0$)
 \item The number of eigenvalues that are returned is specified by \lstinline|num_eigenvalues| (default: $10$)
 \item The size of the krylov space used for the computations can be set by the parameter \lstinline|krylov_size| (default: $100$). The maximum number of iterations can be equal or less this parameter
\end{itemize}
The call of the constructor may look like the following:
\begin{lstlisting}
viennacl::linalg::lanczos_tag ltag(0.85, 15, 0, 200);
\end{lstlisting}

\TIP{Example code can be found in \lstinline|examples/tutorial/lanczos.cpp|.}


\section{QR Factorization}

A matrix $A \in \mathbb{R}^{n\times m}$ can be factored into $A = Q R$, where $Q \in \mathbb{R}^{n\times n}$ is an
orthogonal matrix and $R \in \mathbb{R}^{n \times m}$ is upper triangular. This so-called QR-factorization is important for eigenvalue computations as well as
for the solution of least-squares problems \cite{golub:matrix-computations}. {\ViennaCL} provides a generic implementation of the QR-factorization using
Householder reflections in file \lstinline|viennacl/linalg/qr.hpp|. An example application can be found in \lstinline|examples/tutorial/qr.hpp|.

The Householder reflectors $v_i$ defining the Householder reflection $I - \beta_i v_i v_i^{\mathrm{T}}$ are stored in the
columns below the diagonal of the input matrix $A$ \cite{golub:matrix-computations}. The normalization coefficients $\beta_i$ are returned by the
worker function \lstinline|inplace_qr|. The upper triangular matrix $R$ is directly written to the upper triangular part of $A$. 
\begin{lstlisting}
  std::vector<ScalarType> betas = viennacl::linalg::inplace_qr(A, 12);
\end{lstlisting}
If $A$ is a dense matrix from \ublas, the calculation is carried out on the CPU using a single thread. If $A$ is a 
\lstinline|viennacl::matrix|, a hybrid implementation is used: The panel factorization is carried out using \ublas, while expensive BLAS level 3 operations
are computed on the OpenCL device using multiple threads. 

Typically, the orthogonal matrix $Q$ is kept in inplicit form because of computational efficiency
However, if $Q$ and $R$ have to be computed explicitly, the function \lstinline|recoverQ| can be used:
\begin{lstlisting}
  viennacl::linalg::recoverQ(A, betas, Q, R); 
\end{lstlisting}
Here, \lstinline|A| is the inplace QR-factored matrix, \lstinline|betas| are the coefficients of the Householder reflectors as returned by
\lstinline|inplace_qr|, while \lstinline|Q| and \lstinline|R| are the destination matrices.


\section{Singular Value Decomposition}
\NOTE{Singular Value Decomposition is experimental in {\ViennaCLversion}. Interface changes as well as considerable performance improvements may
be included in future releases!}

\NOTE{Singular Value Decomposition in {\ViennaCLversion} is provided for row-major dense matrices only.}

Any matrix $A$ can be factored as
\begin{align}
 A = U \Sigma V^{\mathrm{T}}
\end{align}
with orthogonal matrices $U$ and $V$ and a diagonal matrix $\Sigma$ consisting of non-negative diagonal entries only.

The implementation in \texttt{viennacl/linalg/svd.hpp} provides the singular value decomposition as follows:
\begin{lstlisting}
 viennacl::matrix<ScalarType> A(size1, size2);
 viennacl::matrix<ScalarType> U(size1, size1);
 viennacl::matrix<ScalarType> V(size2, size2);

 viennacl::linalg::svd(A, U, V);
\end{lstlisting}
The input matrix \lstinline|A| is overwritten with $\Sigma$.

\section{Nonnegative Matrix Factorization}
\NOTE{Nonnegative Matrix Factorization is experimental in {\ViennaCLversion}. Interface changes as well as considerable performance improvements may
be included in future releases!}

In various fields such as text mining, a matrix $V$ needs to be factored into factors $W$ and $H$ such that the function
\begin{align*}
 f(W, H) = \Vert V - WH \Vert_{\mathrm{F}}^2
\end{align*}
is minimized. The algorithm proposed by Lee and Seoung \cite{lee:nmf} is available in ViennaCL in the header file \texttt{viennacl/linalg/nmf.hpp} as
\begin{lstlisting}
 viennacl::matrix<ScalarType> V(size1, size2);
 viennacl::matrix<ScalarType> W(size1, k);
 viennacl::matrix<ScalarType> H(k, size2);

 viennacl::linalg::nmf_config conf;
 viennacl::linalg::nmf(v_ref, w_nmf, h_nmf, conf);
\end{lstlisting}
For an overview of the parameters (tolerances) of the configuration object \lstinline|conf|, please refer to the Doxygen documentation in \texttt{doc/doxygen/}.
